{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae4c563-cd1c-45c1-af96-7d45ba03a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e7d5d42-6912-4db9-bad0-40629451f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_Chinese(uchar):\n",
    "    if uchar >= u'\\u4e00' and uchar <= u'\\u9fa5':\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b319c11-8b52-4bdf-a1fe-bc167e1c2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理中文的方法\n",
    "#包括：句子切分、去标点、去停词、\n",
    "class preprocess_chinese:\n",
    "    def __init__(self):\n",
    "        self.record = []\n",
    "        self.stopwords = []\n",
    "        \n",
    "    #导入停用词\n",
    "    def load_stopwords(self, filename = '../停用词表_w2v/停用词库.txt'):\n",
    "        f_in = open(filename, 'r', encoding = 'utf-8')\n",
    "        for line in f_in:\n",
    "            self.stopwords.append(line.strip())\n",
    "        f_in.close()\n",
    "        \n",
    "    #移除非中文词\n",
    "    #输入: 中文句子;\n",
    "    #输出: 中文句子。\n",
    "    def remove_nonchinese(self, sentence):\n",
    "        precessed_sentence = ''\n",
    "        for char in sentence:\n",
    "            if is_Chinese(char):\n",
    "                precessed_sentence += char\n",
    "        return precessed_sentence        \n",
    "    \n",
    "    #从一个句子里面去除停用词\n",
    "    #输入：句子\n",
    "    #输出：去除停用词之后的 list\n",
    "    def remove_stopwords(self, sentence):\n",
    "        if len(self.stopwords) == 0:\n",
    "            self.load_stopwords()        \n",
    "        segmentation = jieba.lcut(sentence)\n",
    "        removed_sw_list = [word for word in segmentation if not word in self.stopwords]        \n",
    "        return removed_sw_list\n",
    "    \n",
    "    #对一个爬下来的句子进行处理\n",
    "    #split: 正则表达式（方括号表达式，用于句子切分）\n",
    "    #处理步骤：句子切分; 保留中文词; 分词并去停词。 最后会返回一个分完词的 list\n",
    "    #输入: 一个句子\n",
    "    #输出: list(list), list(list); 切分好的句子 以及 切分并处理好的句子\n",
    "    def process_one_sentence(self, ori_sen):\n",
    "        sentence = self.remove_nonchinese(ori_sen)\n",
    "        pre_sentence = self.remove_stopwords(sentence)\n",
    "        return pre_sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d23d711-a1ba-4381-8cab-ebd02e1686fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class create_nontokenize_txt():\n",
    "    def __init__(self):\n",
    "        self.product_ids = None\n",
    "        self.df_ids = None\n",
    "        self.df_list = None\n",
    "    \n",
    "    def _get_ids(self):\n",
    "        product_ids = []\n",
    "        product_ids_path = '../爬虫及其结果/笔记本id.txt'\n",
    "        with open(product_ids_path, 'r') as f:\n",
    "            for line in f:\n",
    "                product_ids += line.split(', ')[:5]\n",
    "        self.product_ids = product_ids\n",
    "        return product_ids\n",
    "    \n",
    "    def _read_data(self):\n",
    "        df_list = []\n",
    "        file_path = '../爬虫及其结果'\n",
    "        df_ids = []\n",
    "        for p_id in self.product_ids:\n",
    "            try:\n",
    "                result_path = os.path.join(file_path, p_id + '.xlsx')\n",
    "                p_df = pd.read_excel(result_path)\n",
    "                df_list.append(p_df)\n",
    "                df_ids.append(p_id)\n",
    "            except:\n",
    "                continue\n",
    "        df = pd.concat(df_list, keys = df_ids)\n",
    "        self.df_list = df_list\n",
    "        self.df_ids = df_ids\n",
    "        return df\n",
    "    \n",
    "    def create_csv(self, path = 'ok.csv'):\n",
    "        df = pd.concat(self.df_list, keys = self.df_ids)\n",
    "        df.to_csv(path)\n",
    "    \n",
    "    def create_txt(self, path = 'train_total.txt'):\n",
    "        if self.product_ids is None:\n",
    "            self._get_ids()\n",
    "        if self.df_ids is None or self.df_list is None:\n",
    "            self._read_data()\n",
    "        report_dict = {}\n",
    "        for i in range(len(self.df_list)):            \n",
    "            for content in self.df_list[i]['内容']:\n",
    "                ori_sen = content    \n",
    "                report_dict['商品id'] = self.df_ids[i]\n",
    "                preprocessor = preprocess_chinese()\n",
    "                pre_sentence = preprocessor.process_one_sentence(ori_sen)      \n",
    "                report_dict['原文本'] = ori_sen\n",
    "                report_dict['处理后的文本'] = pre_sentence\n",
    "                with open(path, 'a', encoding = 'utf-8') as f:\n",
    "                    json.dump(report_dict, f, ensure_ascii=False)\n",
    "                    f.write('\\n')\n",
    "            print(f'{i}/{len(self.df_list)}\\r')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "797cd120-6e9a-4b91-b726-1a4a3b409249",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/145\n",
      "1/145\n",
      "2/145\n",
      "3/145\n",
      "4/145\n",
      "5/145\n",
      "6/145\n",
      "7/145\n",
      "8/145\n",
      "9/145\n",
      "10/145\n",
      "11/145\n",
      "12/145\n",
      "13/145\n",
      "14/145\n",
      "15/145\n",
      "16/145\n",
      "17/145\n",
      "18/145\n",
      "19/145\n",
      "20/145\n",
      "21/145\n",
      "22/145\n",
      "23/145\n",
      "24/145\n",
      "25/145\n",
      "26/145\n",
      "27/145\n",
      "28/145\n",
      "29/145\n",
      "30/145\n",
      "31/145\n",
      "32/145\n",
      "33/145\n",
      "34/145\n",
      "35/145\n",
      "36/145\n",
      "37/145\n",
      "38/145\n",
      "39/145\n",
      "40/145\n",
      "41/145\n",
      "42/145\n",
      "43/145\n",
      "44/145\n",
      "45/145\n",
      "46/145\n",
      "47/145\n",
      "48/145\n",
      "49/145\n",
      "50/145\n",
      "51/145\n",
      "52/145\n",
      "53/145\n",
      "54/145\n",
      "55/145\n",
      "56/145\n",
      "57/145\n",
      "58/145\n",
      "59/145\n",
      "60/145\n",
      "61/145\n",
      "62/145\n",
      "63/145\n",
      "64/145\n",
      "65/145\n",
      "66/145\n",
      "67/145\n",
      "68/145\n",
      "69/145\n",
      "70/145\n",
      "71/145\n",
      "72/145\n",
      "73/145\n",
      "74/145\n",
      "75/145\n",
      "76/145\n",
      "77/145\n",
      "78/145\n",
      "79/145\n",
      "80/145\n",
      "81/145\n",
      "82/145\n",
      "83/145\n",
      "84/145\n",
      "85/145\n",
      "86/145\n",
      "87/145\n",
      "88/145\n",
      "89/145\n",
      "90/145\n",
      "91/145\n",
      "92/145\n",
      "93/145\n",
      "94/145\n",
      "95/145\n",
      "96/145\n",
      "97/145\n",
      "98/145\n",
      "99/145\n",
      "100/145\n",
      "101/145\n",
      "102/145\n",
      "103/145\n",
      "104/145\n",
      "105/145\n",
      "106/145\n",
      "107/145\n",
      "108/145\n",
      "109/145\n",
      "110/145\n",
      "111/145\n",
      "112/145\n",
      "113/145\n",
      "114/145\n",
      "115/145\n",
      "116/145\n",
      "117/145\n",
      "118/145\n",
      "119/145\n",
      "120/145\n",
      "121/145\n",
      "122/145\n",
      "123/145\n",
      "124/145\n",
      "125/145\n",
      "126/145\n",
      "127/145\n",
      "128/145\n",
      "129/145\n",
      "130/145\n",
      "131/145\n",
      "132/145\n",
      "133/145\n",
      "134/145\n",
      "135/145\n",
      "136/145\n",
      "137/145\n",
      "138/145\n",
      "139/145\n",
      "140/145\n",
      "141/145\n",
      "142/145\n",
      "143/145\n",
      "144/145\n"
     ]
    }
   ],
   "source": [
    "txt_creator = create_nontokenize_txt()\n",
    "txt_creator.create_txt()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
