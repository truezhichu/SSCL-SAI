{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "867c6e48-71a6-4835-afb8-25f518f2c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from utils import str2bool\n",
    "import os \n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "'''framework'''\n",
    "parser.add_argument('--task', default = 'train',\n",
    "                    help = 'train | test | others')\n",
    "\n",
    "parser.add_argument('--data_dir', default = '../data',\n",
    "                    help = 'data directory')\n",
    "\n",
    "parser.add_argument('--file_train', default = 'train.txt', \n",
    "                    help = 'training data file name')\n",
    "\n",
    "parser.add_argument('--base_model_dir', default = '../nats_results/sscl_models', \n",
    "                    help = 'base model dir')\n",
    "\n",
    "parser.add_argument('--n_epoch', default = 10, type = int, \n",
    "                    help = 'epochs')\n",
    "\n",
    "parser.add_argument('--batch_size', default = 50, type = int, \n",
    "                    help = 'batch size')\n",
    "\n",
    "parser.add_argument('--checkpoint', default = 300, type = int,\n",
    "                    help = 'how often do you want to save model')\n",
    "\n",
    "parser.add_argument('--continue_training', default = True, type = str2bool, \n",
    "                    help = 'do you want to continue train previous model saved in computer')\n",
    "\n",
    "parser.add_argument('--train_base_model', default = False, type = str2bool,\n",
    "                    help = 'do you want to train word embedding')\n",
    "\n",
    "parser.add_argument('--use_optimal_model', default = True, type = str2bool,\n",
    "                    help = 'weather to use optimal model')\n",
    "\n",
    "parser.add_argument('--model_optimal_key', default = '0,0', \n",
    "                    help = 'epoch, batch')\n",
    "\n",
    "#learning_rate and scheduler\n",
    "parser.add_argument('--lr_schedule', default = 'warm-up', \n",
    "                    help = 'warm-up | build-in | None')\n",
    "\n",
    "parser.add_argument('--learning_rate', default = 0.0002, type = float, \n",
    "                    help = 'learning rate')\n",
    "\n",
    "parser.add_argument('--grad_clip', default = 2.0, type = float, \n",
    "                    help = 'clip the gradient norm')\n",
    "\n",
    "parser.add_argument('--step_size', default = 2, type = int, \n",
    "                    help = 'stepLR scheduler decay period')\n",
    "\n",
    "parser.add_argument('--step_decay', default = 0.8, type = int, \n",
    "                    help = 'decay rate')\n",
    "\n",
    "parser.add_argument('--warmup_step', default = 3000, type = int, \n",
    "                    help = 'the step where the learning rate go to top if apply warmup scheduler')\n",
    "\n",
    "parser.add_argument('--model_size', default = 100000, type = int,\n",
    "                    help = 'to modify learning rate')\n",
    "\n",
    "\n",
    "'''user specified argument'''\n",
    "parser.add_argument('--device', default = 'cuda:0',\n",
    "                    help = 'device')\n",
    "\n",
    "#sscl\n",
    "parser.add_argument('--distance', default = 'cosine', \n",
    "                    help = 'method to compute distance')\n",
    "\n",
    "parser.add_argument('--emb_size', default = 128, type = int, \n",
    "                    help = 'embedding size')\n",
    "\n",
    "parser.add_argument('--max_seq_len', default = 30, type = int, \n",
    "                    help = 'maximum sequence length')\n",
    "\n",
    "parser.add_argument('--min_seq_len', default = 3, type = int, \n",
    "                    help = 'minimum sequence length')\n",
    "\n",
    "parser.add_argument('--smooth_factor', default = 0.5, type = float, \n",
    "                    help = 'smooth factor of attention')\n",
    "\n",
    "#word2vec\n",
    "parser.add_argument('--file_train_w2v', default = 'train_w2v.txt', \n",
    "                    help = 'to train word embedding(file)')\n",
    "\n",
    "parser.add_argument('--window', default = 5, type = int,\n",
    "                    help = 'window size')\n",
    "\n",
    "parser.add_argument('--min_count', default = 10, type = int, \n",
    "                    help = 'the minimum count of word')\n",
    "\n",
    "parser.add_argument('--workers', default = 8, type = int, \n",
    "                    help = 'workers')\n",
    "\n",
    "#kmeans\n",
    "parser.add_argument('--kmeans_init', default = 'vanilla', \n",
    "                    help = 'vanilla | kmeans_init.txt')\n",
    "\n",
    "parser.add_argument('--kmeans_seeds', default = 0, type = int, \n",
    "                    help = 'kmeans random seed')\n",
    "\n",
    "parser.add_argument('--n_clusters', default = 30, type = int, \n",
    "                    help = 'the number of clusters')\n",
    "\n",
    "parser.add_argument('--n_keywords', default = 10, type = int, \n",
    "                    help = 'the number of closest words to choose')\n",
    "\n",
    "#report\n",
    "parser.add_argument('--report', default = ['loss', 'lr', 'loss_without_reg', 'loss_reg', 'asp_weight_norm'], type = list, \n",
    "                    help  = 'weather to report [\"loss\", \"lr\", \"loss_without_reg\", \"loss_reg\"]')\n",
    "\n",
    "parser.add_argument('--monitor', default = ['asp_weight', 'attention'], type = list,\n",
    "                    help = 'weather to moniter p_norm or attention_norm')\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d213a3dd-c6fd-477e-b752-301b6c8819ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_task(args):\n",
    "    if args.task == 'word2vec':\n",
    "        from word2vec import run_w2v\n",
    "        runner = run_w2v(args)\n",
    "        model = runner.train_model()\n",
    "        runner.save_vocab_and_vectors()\n",
    "    \n",
    "    if args.task[:6] == 'kmeans':\n",
    "        if args.task == 'kmeans':\n",
    "            from kmeans import run_kmeans\n",
    "            runner = run_kmeans(args)\n",
    "            runner.train()\n",
    "        \n",
    "    if args.task[:4] == 'sscl':\n",
    "        import torch\n",
    "        args.device = torch.device(args.device)\n",
    "        from modelSSCL import modelSSCL\n",
    "        model = modelSSCL(args)\n",
    "    \n",
    "        if args.task == 'sscl-train':\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e786b7b4-b7e0-459c-b0d4-83457dff8f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"args.task = 'word2vec'\\nrun_task(args)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.task = 'word2vec'\n",
    "run_task(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47987009-c4b2-4e29-8da7-3ade19f9dcdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"args.task = 'kmeans'\\nrun_task(args)\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.task = 'kmeans'\n",
    "run_task(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b293f1dc-0178-47d8-b8c4-c754efc6d926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of vocabulary :6273.\n",
      "{'embedding': Embedding(6273, 128)}\n",
      "{'asp_weight': Linear(in_features=128, out_features=30, bias=True),\n",
      " 'aspect_embedding': Embedding(30, 128, padding_idx=0),\n",
      " 'attn_kernel': Linear(in_features=128, out_features=128, bias=True)}\n",
      "Total number of trainable parameters 24222.\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:0, batch:5955/5958, lr:8.2e-05, loss:3.1149, time:0.1072hhh\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:1, batch:5955/5958, lr:5.8e-05, loss:3.2104, time:0.2215h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:2, batch:5955/5958, lr:4.7e-05, loss:3.2529, time:0.3423h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:3, batch:5955/5958, lr:4.1e-05, loss:2.9563, time:0.4636h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:4, batch:5955/5958, lr:3.7e-05, loss:3.2142, time:0.584hh\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:5, batch:5955/5958, lr:3.3e-05, loss:3.1266, time:0.7046h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:6, batch:5955/5958, lr:3.1e-05, loss:3.1958, time:0.8252h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:7, batch:5955/5958, lr:2.9e-05, loss:3.3273, time:0.9465h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:8, batch:5955/5958, lr:2.7e-05, loss:3.2684, time:1.0665h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:9, batch:5955/5958, lr:2.6e-05, loss:3.1656, time:1.1824h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:10, batch:5955/5958, lr:2.5e-05, loss:3.3312, time:1.2867h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:11, batch:5955/5958, lr:2.4e-05, loss:3.2591, time:1.3916h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:12, batch:5955/5958, lr:2.3e-05, loss:3.1245, time:1.4963h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:13, batch:5955/5958, lr:2.2e-05, loss:3.2912, time:1.6014h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:14, batch:5955/5958, lr:2.1e-05, loss:3.1063, time:1.7056h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:15, batch:5955/5958, lr:2e-05, loss:3.2892, time:1.8123h7h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:16, batch:5955/5958, lr:2e-05, loss:3.2614, time:1.9195h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:17, batch:5955/5958, lr:1.9e-05, loss:3.244, time:2.0245hh\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:18, batch:5955/5958, lr:1.9e-05, loss:3.2409, time:2.1298h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:19, batch:5955/5958, lr:1.8e-05, loss:3.1335, time:2.2459h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:20, batch:5955/5958, lr:1.8e-05, loss:3.0394, time:2.3683h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:21, batch:5955/5958, lr:1.7e-05, loss:3.2957, time:2.4898h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:22, batch:5955/5958, lr:1.7e-05, loss:3.1929, time:2.6132h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:23, batch:5955/5958, lr:1.7e-05, loss:3.2167, time:2.7289h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:24, batch:5955/5958, lr:1.6e-05, loss:3.2334, time:2.837hh\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:25, batch:5955/5958, lr:1.6e-05, loss:3.2373, time:2.9499h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:26, batch:5955/5958, lr:1.6e-05, loss:2.9999, time:3.0662h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:27, batch:5955/5958, lr:1.5e-05, loss:3.2489, time:3.1753h\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:28, batch:5955/5958, lr:1.5e-05, loss:3.203, time:3.2772hh\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:29, batch:5955/5958, lr:1.5e-05, loss:3.227, time:3.3822hh\n"
     ]
    }
   ],
   "source": [
    "args.task = 'sscl-train'\n",
    "run_task(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
