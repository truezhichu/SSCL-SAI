{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "867c6e48-71a6-4835-afb8-25f518f2c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from utils import str2bool\n",
    "import os \n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "'''framework'''\n",
    "\n",
    "parser.add_argument('--task', default = 'train',\n",
    "                    help = 'train | test | others')\n",
    "\n",
    "parser.add_argument('--data_dir', default = '../data',\n",
    "                    help = 'data directory')\n",
    "\n",
    "parser.add_argument('--file_train', default = 'train.txt', \n",
    "                    help = 'training data file name')\n",
    "\n",
    "parser.add_argument('--base_model_dir', default = '../nats_results/sscl_models', \n",
    "                    help = 'base model dir')\n",
    "\n",
    "parser.add_argument('--n_epoch', default = 3, type = int, \n",
    "                    help = 'epochs')\n",
    "\n",
    "parser.add_argument('--batch_size', default = 50, type = int, \n",
    "                    help = 'batch size')\n",
    "\n",
    "parser.add_argument('--checkpoint', default = 300, type = int,\n",
    "                    help = 'how often do you want to save model')\n",
    "\n",
    "parser.add_argument('--continue_training', default = True, type = str2bool, \n",
    "                    help = 'do you want to continue train previous model saved in computer')\n",
    "\n",
    "parser.add_argument('--train_base_model', default = False, type = str2bool,\n",
    "                    help = 'do you want to train word embedding')\n",
    "\n",
    "parser.add_argument('--use_optimal_model', default = True, type = str2bool,\n",
    "                    help = 'weather to use optimal model')\n",
    "\n",
    "parser.add_argument('--model_optimal_key', default = '0,0', \n",
    "                    help = 'epoch, batch')\n",
    "\n",
    "#learning_rate and scheduler\n",
    "parser.add_argument('--lr_schedule', default = 'warm-up', \n",
    "                    help = 'warm-up | build-in | None')\n",
    "\n",
    "parser.add_argument('--learning_rate', default = 0.0002, type = float, \n",
    "                    help = 'learning rate')\n",
    "\n",
    "parser.add_argument('--grad_clip', default = 2.0, type = float, \n",
    "                    help = 'clip the gradient norm')\n",
    "\n",
    "parser.add_argument('--step_size', default = 2, type = int, \n",
    "                    help = 'stepLR scheduler decay period')\n",
    "\n",
    "parser.add_argument('--step_decay', default = 0.8, type = int, \n",
    "                    help = 'decay rate')\n",
    "\n",
    "parser.add_argument('--warmup_step', default = 3000, type = int, \n",
    "                    help = 'the step where the learning rate go to top if apply warmup scheduler')\n",
    "\n",
    "parser.add_argument('--model_size', default = 100000, type = int,\n",
    "                    help = 'to modify learning rate')\n",
    "\n",
    "\n",
    "'''user specified argument'''\n",
    "parser.add_argument('--device', default = 'cuda:0',\n",
    "                    help = 'device')\n",
    "\n",
    "#sscl\n",
    "parser.add_argument('--distance', default = 'cosine', \n",
    "                    help = 'method to compute distance')\n",
    "\n",
    "parser.add_argument('--emb_size', default = 128, type = int, \n",
    "                    help = 'embedding size')\n",
    "\n",
    "parser.add_argument('--max_seq_len', default = 30, type = int, \n",
    "                    help = 'maximum sequence length')\n",
    "\n",
    "parser.add_argument('--min_seq_len', default = 3, type = int, \n",
    "                    help = 'minimum sequence length')\n",
    "\n",
    "parser.add_argument('--smooth_factor', default = 0.5, type = float, \n",
    "                    help = 'smooth factor of attention')\n",
    "\n",
    "# 消融实验\n",
    "parser.add_argument('--if_init_kmeans', default = True, type = str2bool, \n",
    "                    help = 'weather init kmeans')\n",
    "\n",
    "parser.add_argument('--if_reg', default = True, type = str2bool, \n",
    "                    help = 'weather orthogonal regularization')\n",
    "\n",
    "#word2vec\n",
    "parser.add_argument('--file_train_w2v', default = 'train_w2v.txt', \n",
    "                    help = 'to train word embedding(file)')\n",
    "\n",
    "parser.add_argument('--window', default = 5, type = int,\n",
    "                    help = 'window size')\n",
    "\n",
    "parser.add_argument('--min_count', default = 10, type = int, \n",
    "                    help = 'the minimum count of word')\n",
    "\n",
    "parser.add_argument('--workers', default = 8, type = int, \n",
    "                    help = 'workers')\n",
    "\n",
    "#kmeans\n",
    "parser.add_argument('--kmeans_init', default = 'vanilla', \n",
    "                    help = 'vanilla | kmeans_init.txt')\n",
    "\n",
    "parser.add_argument('--kmeans_seeds', default = 0, type = int, \n",
    "                    help = 'kmeans random seed')\n",
    "\n",
    "parser.add_argument('--n_clusters', default = 30, type = int, \n",
    "                    help = 'the number of clusters')\n",
    "\n",
    "parser.add_argument('--n_keywords', default = 10, type = int, \n",
    "                    help = 'the number of closest words to choose')\n",
    "\n",
    "#report\n",
    "parser.add_argument('--report', default = ['loss', 'lr', 'loss_without_reg', 'loss_reg', 'asp_weight_norm'], type = list, \n",
    "                    help  = 'weather to report [\"loss\", \"lr\", \"loss_without_reg\", \"loss_reg\"]')\n",
    "\n",
    "parser.add_argument('--monitor', default = ['asp_weight', 'attention'], type = list,\n",
    "                    help = 'weather to moniter p_norm or attention_norm')\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d213a3dd-c6fd-477e-b752-301b6c8819ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_task(args):\n",
    "    if args.task == 'word2vec':\n",
    "        from word2vec import run_w2v\n",
    "        runner = run_w2v(args)\n",
    "        model = runner.train_model()\n",
    "        runner.save_vocab_and_vectors()\n",
    "    \n",
    "    if args.task[:6] == 'kmeans':\n",
    "        if args.task == 'kmeans':\n",
    "            from kmeans import run_kmeans\n",
    "            runner = run_kmeans(args)\n",
    "            runner.train()\n",
    "        \n",
    "    if args.task[:4] == 'sscl':\n",
    "        import torch\n",
    "        args.device = torch.device(args.device)\n",
    "        from modelSSCL import modelSSCL\n",
    "        model = modelSSCL(args)\n",
    "    \n",
    "        if args.task == 'sscl-train':\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e786b7b4-b7e0-459c-b0d4-83457dff8f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhichu\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练词向量。。。\n",
      "共有297882个句子。\n",
      "训练完成！\n",
      "正在写入字典，格式：(单词 索引)\n",
      "写入完成！\n",
      "正在写入词向量(numpy)\n",
      "写入完成！\n"
     ]
    }
   ],
   "source": [
    "args.task = 'word2vec'\n",
    "run_task(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47987009-c4b2-4e29-8da7-3ade19f9dcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在训练模型。。。\n",
      "训练完成！\n",
      "正在保存中心。。。\n",
      "保存完成！\n",
      "正在保存topn = 10 的临近词。。。\n",
      "保存完成！\n"
     ]
    }
   ],
   "source": [
    "args.task = 'kmeans'\n",
    "run_task(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b293f1dc-0178-47d8-b8c4-c754efc6d926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of vocabulary :6273.\n",
      "{'embedding': Embedding(6273, 128)}\n",
      "{'asp_weight': Linear(in_features=128, out_features=30, bias=True),\n",
      " 'aspect_embedding': Embedding(30, 128, padding_idx=0),\n",
      " 'attn_kernel': Linear(in_features=128, out_features=128, bias=True)}\n",
      "Total number of trainable parameters 24222.\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:0, batch:5955/5958, lr:8.2e-05, loss:3.0389, time:0.1342hhh\n",
      "loading data...\n",
      "batch number: 5958\n",
      "writing data...\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>] 100% \n",
      "The number of batch is 5958.\n",
      "epoch:1, batch:5830/5958, lr:5.8e-05, loss:3.2601, time:11.0839h\r"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\nats_results\\\\batch_sscl-train_50\\\\5832'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4256/2321792911.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'sscl-train'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrun_task\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4256/2931607879.py\u001b[0m in \u001b[0;36mrun_task\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sscl-train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\_论文尝试\\聚类 - 副本\\源代码\\End2EndBase.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m                     \u001b[0mcclb\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_pipelines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\_论文尝试\\聚类 - 副本\\源代码\\modelSSCLBase.py\u001b[0m in \u001b[0;36mbuild_batch\u001b[1;34m(self, batch_id)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0msen_text_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[0mfp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0mitm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\nats_results\\\\batch_sscl-train_50\\\\5832'"
     ]
    }
   ],
   "source": [
    "args.task = 'sscl-train'\n",
    "run_task(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
